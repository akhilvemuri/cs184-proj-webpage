<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<style>
  body {
    background-color: white;
    padding: 100px;
    width: 1000px;
    margin: auto;
    text-align: left;
    font-weight: 300;
    font-family: 'Open Sans', sans-serif;
    color: #121212;
  }
  h1, h2, h3, h4 {
    font-family: 'Source Sans Pro', sans-serif;
  }
  kbd {
    color: #121212;
  }
  code {
    font-size: 16px;
  }
</style>
<title>CS 184 Path Tracer</title>
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<link href="https://fonts.googleapis.com/css?family=Open+Sans|Source+Sans+Pro" rel="stylesheet">

<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']]
    }
  };
</script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>

</head>


<body>

    <h1 align="middle">CS 184: Computer Graphics and Imaging, Spring 2023</h1>
    <h1 align="middle">Project 3-1: Path Tracer</h1>
    <h2 align="middle">MEIQI SUN</h2>
    <h2 align="middle">AKHIL VEMURI</h2>

    <!-- Add Website URL -->
    <h2 align="middle">Website URL: <a href="https://akhilvemuri.github.io/cs184-proj-webpage/">https://akhilvemuri.github.io/cs184-proj-webpage/</a></h2>

    <br />


    <div align="center">
        <table style="width=100%">
            <tr>
                <td align="middle">
                    <img src="images/example_image.png" width="480px" />
                </td>
            </tr>
        </table>
    </div>

    <!--p>All of the text in your write-up should be <em>in your own words</em>. If you need to add additional HTML features to this document, you can search the <a href="http://www.w3schools.com/">http://www.w3schools.com/</a> website for instructions. To edit the HTML, you can just copy and paste existing chunks and fill in the text and image file names appropriately.</p>
    <o>
        The website writeup is intended to be a self-contained walkthrough of the assignment: we want this to be a piece of work which showcases your understanding of relevant concepts through both mesh images as well as written explanations about what you did to complete each part of the assignment. Try to be as clear and organized as possible when writing about your own output files or extensions to the assignment. We want to understand what you've achieved and how you've done it!</p>
        <p>If you are well-versed in web development, feel free to ditch this template and make a better looking page.</p>


        <p>Here are a few problems students have encountered in the past. Test your website on the instructional machines early!</p>
        <ul>
            <li>Your main report page should be called index.html.</li>
            <li>Be sure to include and turn in all of the other files (such as images) that are linked in your report!</li>
            <li>
                Use only <em>relative</em> paths to files, such as <pre>"./images/image.jpg"</pre>
                Do <em>NOT</em> use absolute paths, such as <pre>"/Users/student/Desktop/image.jpg"</pre>
            </li>
            <li>Pay close attention to your filename extensions. Remember that on UNIX systems (such as the instructional machines), capitalization matters. <pre>.png != .jpeg != .jpg != .JPG</pre></li>
            <li>Be sure to adjust the permissions on your files so that they are world readable. For more information on this please see this tutorial: <a href="http://www.grymoire.com/Unix/Permissions.html">http://www.grymoire.com/Unix/Permissions.html</a></li>
            <li>And again, test your website on the instructional machines early!</li>
        </ul>


        <p>Here is an example of how to include a simple formula:</p>
        <p align="middle"><pre align="middle">a^2 + b^2 = c^2</pre></p>
        <p>or, alternatively, you can include an SVG image of a LaTex formula.</p-->

    <div>

        <h2 align="middle">Overview</h2>
        <p>
            For this project,we implemented a physically-based renderer using the path tracing algorithm. Our project allows ray generation, computing ray-primitive intersections, applying Bounding Volume Hierarchy to increase rendering speed, Direct Illustration, Global Illumination, and adaptive sampling to more effectively denoise the rendering.

        </p>
        <br />

        <h2 align="middle">Part 1: Ray Generation and Scene Intersection (20 Points)</h2>
        <!-- Walk through the ray generation and primitive intersection parts of the rendering pipeline.
    Explain the triangle intersection algorithm you implemented in your own words.
    Show images with normal shading for a few small .dae files. -->

        <h3>
            Walk through the ray generation and primitive intersection parts of the rendering pipeline.
        </h3>
        <p>
            <b>Ray Generation:</b> <br />
            The basic idea of ray generation is to emit a ray from the origin to each (or a selected subset of samples of) pixels. In this project specifically, given a normalized image coordinate, we can convert the point from image space to sensor in the camera space, and generate a ray that originates from the camera origin and extends along the camera ray direction. Then, we can transform it into a ray in world space using the c2w matrix. This is the basis of the rendering pipeline, because only after we generate rays can we test the intersection between rays and primitives.
            <br />
            <br />
            <b>Primitive Intersection:</b> <br />
            After we generate a ray, we want to test if there’s any intersection between the ray and our primitive. If so, we want to find the closest intersection point/can also be used to calculate shadows. In this problem, we implemented ray triangle intersection and ray sphere intersection. The basic idea is to set the ray equation to be equal to the primitive equation, and solve for the intersection point.

        </p>
        <br />

        <h3>
            Explain the triangle intersection algorithm you implemented in your own words.
        </h3>
        <p>
            <b>Triangle Intersection</b>

            <ol style="list-style-type: disc;">
                <li>
                    First, we implemented the Moller Trumbore Algorithm, which, given the origin and direction vector of the ray and the vertices of the triangles, computes the corresponding intersection for us in terms of t, b1, and b2. In essence, the algorithm computes the intersection point (solves the intersection equation) more efficiently.
                </li>
                <li>
                    Then, construct barycentric coordinates (b1, b2, 1-b1-b2) and check whether they are all positive to determine if the intersection is actually inside the triangle. We also need to check if t is within the range of ray.min_t and ray.max_t.

                </li>

                <li>
                    If all the conditions are met, we update the ray’s max_t to the t-value of the current intersection point, and update the intersection object accordingly. The value of t follows directly from the Moller Trumbore Algorithm. The surface normal n can be interpolated using barycentric coordinates. And primitive/bsdf are also relatively simple to compute.
                </li>

            </ol>
        </p>
        <br />

        <h3>
            Show images with normal shading for a few small .dae files.
        </h3>
        <!-- Example of including multiple figures -->
        <div align="middle">
            <table style="width:100%">
                <tr align="center">
                    <td>
                        <img src="images/task1/CBgems.png" align="middle" width="400px" />
                        <figcaption>Fig 1: CBgems.dae</figcaption>
                    </td>
                    <td>
                        <img src="images/task1/CBspheres.png" align="middle" width="400px" />
                        <figcaption>Fig 2: CBspheres.dae</figcaption>
                    </td>
                </tr>
                <tr align="center">
                    <td>
                        <img src="images/task1/cube.png" align="middle" width="400px" />
                        <figcaption>Fig 3: cube.dae</figcaption>
                    </td>
                    <td>
                        <img src="images/task1/teapot.png" align="middle" width="400px" />
                        <figcaption>Fig 4: teapot.dae</figcaption>
                    </td>
                </tr>
            </table>
        </div>
        <br />


        <h2 align="middle">Part 2: Bounding Volume Hierarchy (20 Points)</h2>
        <!-- Walk through your BVH construction algorithm. Explain the heuristic you chose for picking the splitting point.
    Show images with normal shading for a few large .dae files that you can only render with BVH acceleration.
    Compare rendering times on a few scenes with moderately complex geometries with and without BVH acceleration. Present your results in a one-paragraph analysis. -->

        <h3>
            Walk through your BVH construction algorithm. Explain the heuristic you chose for picking the splitting point.
        </h3>
        <p>
            <b>For the BVH construction, we:</b>

            <ol style="list-style-type: disc;">
                <li>
                    First iterated over all the primitives and created a bounding box that fits all the primitives. Simultaneously, we counted the number of elements from start to end
                </li>
                <li>
                    Then we initialized the BVHNode with the bounding box
                </li>

                <li>
                    We determined if the number of elements is less than the max_leaf_size. If so, we set the nodes’ start and end to the input start and end correspondingly, and directly returns the node
                </li>
                <li>
                    If we have more primitives than max_leaf_size, we need to compute the left and right node. We find the best split axis and split threshold (heuristic explained below). We then sort the primitives according to the position of the centroid of their bounding box along the selected axis. Next, we iterate over the primitives to group them by whether their target axis coordinate is less or greater than the threshold. Get the boundary element.
                </li>
                <li>
                    Last, we recursively call contruct_bvh to get the left (icluding start to primitives before boundary) and right node (including boundary element to end primitive).
                </li>

            </ol>




            <b>
                Heuristic Explanation:
            </b><br />
            In short, we used the axis mean heuristic. Specifically, we:

            <ol style="list-style-type: disc;">
                <li>
                    First, we computed the Vector3D mean of the centroid of the bounding boxes of all primitives (along x, y, z axis)
                </li>
                <li>
                    Then, we calculated the expected number of primitive that will fall to the left node if we were to split along each axis with mean as the threshold: x_cnt, y_cnt, z_cnt
                </li>

                <li>
                    Since we want an even split, we then find the axis that gives us the highest (total_cnt - left_cnt) * left_cnt. Basic idea: when the circumference of a rectangle is fixed, then square gives the largest area.
                </li>
                <li>
                    Now, we get the target axis, and use the mean of all primitives’ coordinates along this axis as the left right split threshold.
                </li>

            </ol>

        </p>
        <br />

        <h3>
            Show images with normal shading for a few large .dae files that you can only render with BVH acceleration.
        </h3>
        <!-- Example of including multiple figures -->
        <div align="middle">
            <table style="width:100%">
                <tr align="center">
                    <td>
                        <img src="images/task2/beast.png" align="middle" width="400px" />
                        <figcaption>Fig 5: beast.dae</figcaption>
                    </td>
                    <td>
                        <img src="images/task2/peter.png" align="middle" width="400px" />
                        <figcaption>Fig 6: peter.dae</figcaption>
                    </td>
                </tr>
                <tr align="center">
                    <td>
                        <img src="images/task2/maxplanck.png" align="middle" width="400px" />
                        <figcaption>Fig 7: maxplank.dae</figcaption>
                    </td>
                    <td>
                        <img src="images/task2/blob.png" align="middle" width="400px" />
                        <figcaption>Fig 8: blob.dae</figcaption>
                    </td>
                </tr>
            </table>
        </div>
        <br />

        <h3>
            Compare rendering times on a few scenes with moderately complex geometries with and without BVH acceleration. Present your results in a one-paragraph analysis.
        </h3>

        <div align="middle">
            <table style="width:100%">
                <tr align="center">
                    <td>
                        <img src="images/task2/non_bvh_beetles.png" align="middle" width="400px" />
                        <figcaption>Fig 9: No BVH beetles</figcaption>
                    </td>
                    <td>
                        <img src="images/task2/bvh_beetles.png" align="middle" width="400px" />
                        <figcaption>Fig 10: No BVH beetles</figcaption>
                    </td>
                </tr>
                <tr align="center">
                    <td>
                        <img src="images/task2/non_bvh_bunny.png" align="middle" width="400px" />
                        <figcaption>Fig 11: No BVH bunny</figcaption>
                    </td>
                    <td>
                        <img src="images/task2/bvh_bunny.png" align="middle" width="400px" />
                        <figcaption>Fig 12: BVH bunny</figcaption>
                    </td>
                </tr>
            </table>
        </div>
        <br />
        <p>
            We found BV rendering significantly faster than the non-BVH version. For a ~1800 KB “large image”, the naive implementation takes around 500 seconds to render, whereas BVH only takes less than one second.
            We think this is because the naive implementation is checking the intersection between the ray and each single primitive, trying to find the closest intersection. However, with a tree structure, we significantly reduce the amount of computation: when the ray doesn’t intersect with the bounding box, we can trim the entire node without performing a ray-primitive intersection test.
            Even if the ray intersects with a specific bounding box, we can still recursively examine the left node/right node, and might still decrease the scope of our ray-primitive intersection calculations. In environments where there’s more empty space, this can be very efficient and convenient compared to the naive implementation.
        </p>
        <br />

        <h2 align="middle">Part 3: Direct Illumination (20 Points)</h2>
        <!-- Walk through both implementations of the direct lighting function.
    Show some images rendered with both implementations of the direct lighting function.
    Focus on one particular scene with at least one area light and compare the noise levels in soft shadows when rendering with 1, 4, 16, and 64 light rays (the -l flag) and with 1 sample per pixel (the -s flag) using light sampling, not uniform hemisphere sampling.
    Compare the results between uniform hemisphere sampling and lighting sampling in a one-paragraph analysis. -->

        <h3>
            Walk through both implementations of the direct lighting function.
        </h3>
        <p>
            <ul>
                <li>
                    For uniform hemisphere sampling, we sample the incident ray at a hit point randomly since we don't know the position of the light source.
                    If the incident ray doesn't intersect with any object, we discard the sample.
                    If the ray intersects with some objects in the scene, we calculate how much light is emitting from that intersection point and scale it using Monte Carlo Estimation to get its contribution to our primary ray ($L_{out}$, output ray from hit point).
                </li>
                <li>
                    For importance sampling, we generate a shadow ray originating from the hit point, with a small offset ($EPS\_F$) added to the ray's $min\_t$ field to alleviate floating point imprecision.
                    We then check if the ray intersects with any objects before reaching the light's position that is currently being sampled.
                    If the light is a point light, all samples fall on the same location, so only one sample is taken. However, if the light is not a point light, the number of samples is set via the "$-l$ <INT>
                        " command line parameter.
                        If the shadow ray intersects with an object, the sample is discarded. If the shadow ray does not intersect with any objects, we perform Monte Carlo Estimation.
                        For each light, we compute the average of all samples that do not intersect with any objects and add it to the final output ray $L_{out}$.
                    </INT>
                </li>
                <li>
                    In order to carry out Monte Carlo Estimation for both direct lighting functions, we calculate the BSDF value, multiply it by $cos\_theta(w_{in})$, divide it by the pdf, and average that value across $n$ samples.
                </li>
            </ul>
        </p>
        <br />

        <h3>
            Show some images rendered with both implementations of the direct lighting function.
        </h3>
        <p>
            The uniform hemisphere sampled images below were rendered with parameters: <code>-s 64 -l 32 -H</code>.
            <br />
            The lighting sampled images below were rendered with parameters: <code>-s 64 -l 32</code>.
        </p>
        <!-- Example of including multiple figures -->
        <div align="middle">
            <table style="width:100%">
                <!-- Header -->
                <tr align="center">
                    <th>
                        <b>Uniform Hemisphere Sampling</b>
                    </th>
                    <th>
                        <b>Light Sampling</b>
                    </th>
                </tr>
                <tr align="center">
                    <td>
                        <img src="images/task3/bunny_H_64_32.png" align="middle" width="400px" />
                        <figcaption>Fig 13: CBbunny.dae</figcaption>
                    </td>
                    <td>
                        <img src="images/task3/bunny_64_32.png" align="middle" width="400px" />
                        <figcaption>Fig 14: CBbunny.dae</figcaption>
                    </td>
                </tr>
                <tr align="center">
                    <td>
                        <img src="images/task3/spheres_H_64_32.png" align="middle" width="400px" />
                        <figcaption>Fig 15: CBspheres_lambertian.dae</figcaption>
                    </td>
                    <td>
                        <img src="images/task3/spheres_64_32.png" align="middle" width="400px" />
                        <figcaption>Fig 16: CBspheres_lambertian.dae</figcaption>
                    </td>
                </tr>
            </table>
        </div>
        <br />

        <h3>
            Focus on one particular scene with at least one area light and compare the noise levels in <b>soft shadows</b> when rendering with 1, 4, 16, and 64 light rays (the -l flag) and with 1 sample per pixel (the -s flag) using light sampling, <b>not</b> uniform hemisphere sampling.
        </h3>
        <p>
            The images below were rendered with parameters: <code>-s 1 -l [1, 4, 16, 64]</code>.
        </p>
        <!-- Example of including multiple figures -->
        <div align="middle">
            <table style="width:100%">
                <tr align="center">
                    <td>
                        <img src="images/task3/l1_1_1.png" align="middle" width="400px" />
                        <figcaption>Fig 17: 1 Light Ray (CBbunny.dae)</figcaption>
                    </td>
                    <td>
                        <img src="images/task3/l4_1_4.png" align="middle" width="400px" />
                        <figcaption>Fig 18: 4 Light Rays (CBbunny.dae)</figcaption>
                    </td>
                </tr>
                <tr align="center">
                    <td>
                        <img src="images/task3/l16_1_16.png" align="middle" width="400px" />
                        <figcaption>Fig 19: 16 Light Rays (CBbunny.dae)</figcaption>
                    </td>
                    <td>
                        <img src="images/task3/l64_1_64.png" align="middle" width="400px" />
                        <figcaption>Fig 20: 64 Light Rays (CBbunny.dae)</figcaption>
                    </td>
                </tr>
            </table>
        </div>
        <p>
            As we increase the number of light rays, the noise levels in the soft shadows decrease. This is because each additional light ray contributes more information to the overall image.
            When only a few light rays are used, the lighting at a point may vary greatly from sample to sample due to the random nature of the sampling process. This variation can cause visible noise in the final image.
            However, as more light rays are used, the random variation in lighting at each point is reduced because the samples become more representative of the true lighting distribution. With enough light rays, the random noise in the final image is reduced to the point where the noise is no longer visible.
            But, there are also diminishing returns as we add more light rays, which can be seen in the transition from Fig 19 to Fig 20.
        </p>
        <br />

        <h3>
            Compare the results between uniform hemisphere sampling and lighting sampling in a one-paragraph analysis.
        </h3>
        <p>
            Uniform hemisphere sampling randomly samples points on a hemisphere oriented according to the surface normal, while lighting sampling uses a distribution to concentrate the sampling in areas where there is a higher contribution to the final lighting.
            In general, lighting sampling tends to produce more accurate and smoother results than uniform hemisphere sampling. Uniform sampling also tends to take higher samples_per_pixel and rays_per_pixel to converge than importance / lighting sampling.
            This makes lighting sampling especially useful in scenes with soft shadows, where uniform hemisphere sampling may result in noisy or inaccurate results.
            However, lighting sampling can be more computationally expensive than uniform hemisphere sampling, particularly if the scene has many light sources or if the light sources are large or complex.
        </p>
        <br />


        <h2 align="middle">Part 4: Global Illumination (20 Points)</h2>
        <!-- Walk through your implementation of the indirect lighting function.
    Show some images rendered with global (direct and indirect) illumination. Use 1024 samples per pixel.
    Pick one scene and compare rendered views first with only direct illumination, then only indirect illumination. Use 1024 samples per pixel. (You will have to edit PathTracer::at_least_one_bounce_radiance(...) in your code to generate these views.)
    For CBbunny.dae, compare rendered views with max_ray_depth set to 0, 1, 2, 3, and 100 (the -m flag). Use 1024 samples per pixel.
    Pick one scene and compare rendered views with various sample-per-pixel rates, including at least 1, 2, 4, 8, 16, 64, and 1024. Use 4 light rays.
    You will probably want to use the instructional machines for the above renders in order to not burn up your own computer for hours. -->

        <h3>
            Walk through your implementation of the indirect lighting function.
        </h3>
        <p>
            <ul>
                <li>
                    To estimate the total radiance with global illumination arriving at a specific point from a given direction, we call the function $est\_radiance\_global\_illumination$.
                    If no bounces of light occur, the function $zero\_bounce\_radiance$ simply returns the light emitted by the given point along its outgoing direction, meaning a pixel will be black unless it lies under a light.
                    We already implemented this and the function $one\_bounce\_radiance$ to calculate direct illumination in Part 3, which only needs to be called according to whether or not we use direct illumination within our global illumination scheme.
                </li>
                <li>
                    The function $at\_least\_one\_bounce\_radiance$ calls $one\_bounce\_radiance$ and then recursively calls itself to estimate higher bounces. To achieve this, we take one random sample from the BSDF at the hit point, trace a ray in that sample direction, and recursively call the function on the new hit point.
                    However, to improve performance and maintain an unbiased estimation, we use Russian Roulette and only allow rays to go into deeper recursions according to some continuation probability $cpdf$. We control the maximum levels of recursion by setting a parameter called $max\_ray\_depth$.
                </li>
            </ul>
        </p>
        <br />

        <h3>
            Show some images rendered with global (direct and indirect) illumination. Use 1024 samples per pixel.
        </h3>
        <p>
            The images below were rendered with parameters: <code>-s 1024 -l 1 -m 3</code>.
        </p>
        <!-- Example of including multiple figures -->
        <div align="middle">
            <table style="width:100%">
                <tr align="center">
                    <td>
                        <img src="images/task4/global1_1024_1_3.png" align="middle" width="400px" />
                        <figcaption>Fig 21: CBbunny.dae</figcaption>
                    </td>
                    <td>
                        <img src="images/task4/global2_1024_1_3.png" align="middle" width="400px" />
                        <figcaption>Fig 22: CBspheres_lambertian.dae</figcaption>
                    </td>
                </tr>
            </table>
        </div>
        <br />

        <h3>
            Pick one scene and compare rendered views first with only direct illumination, then only indirect illumination. Use 1024 samples per pixel. (You will have to edit PathTracer::at_least_one_bounce_radiance(...) in your code to generate these views.)
        </h3>
        <p>
            The images below were rendered with parameters: <code>-s 1024 -l 4 -m [1, 3]</code>.
        </p>
        <!-- Example of including multiple figures -->
        <div align="middle">
            <table style="width:100%">
                <tr align="center">
                    <td>
                        <img src="images/task4/direct_only_1024_4_1.png" align="middle" width="400px" />
                        <figcaption>Fig 23: Only direct illumination (CBspheres_lambertian.dae)</figcaption>
                    </td>
                    <td>
                        <img src="images//task4/indirect_only_1024_4_3.png" align="middle" width="400px" />
                        <figcaption>Fig 24: Only indirect illumination (CBspheres_lambertian.dae)</figcaption>
                    </td>
                </tr>
            </table>
        </div>
        <br />
        <p>
            Direct illumination refers to the light that comes directly from light sources and illuminates the surfaces in the scene. Indirect illumination, on the other hand, refers to the light that is reflected off surfaces in the scene and bounces around before finally reaching the camera.
            In order to render <i>only</i> indirect illumination, we had to adapt our code by splitting $L_{out}$ into separate direct and indirect calculations.
            The indirect illumination calculation came from subtracting $zero\_bounce\_radiance$ and $one\_bounce\_radiance$ from $at\_least\_one\_bounce\_radiance$, which allowed us to isolate indirect lighting (everything except zero <i>and</i> one bounce).
            When comparing the rendered views, we can observe the differences in the lighting and shadow effects. The images rendered with only direct illumination have sharp and well-defined shadows, while the images rendered with only indirect illumination have softer and more diffuse shadows.
            The images rendered with indirect illumination also have more subtle variations in color and brightness as the light bounces around the scene and interacts with the surfaces.
        </p>
        <br />

        <h3>
            For CBbunny.dae, compare rendered views with max_ray_depth set to 0, 1, 2, 3, and 100 (the -m flag). Use 1024 samples per pixel.
        </h3>
        <p>
            The images below were rendered with parameters: <code>-s 1024 -l 1 -m [0, 1, 2, 3, 100]</code>.
        </p>
        <!-- Example of including multiple figures -->
        <div align="middle">
            <table style="width:100%">
                <tr align="center">
                    <td>
                        <img src="images/task4/m0_1024_1_0.png" align="middle" width="400px" />
                        <figcaption>Fig 25: max_ray_depth = 0 (CBbunny.dae)</figcaption>
                    </td>
                    <td>
                        <img src="images/task4/m1_1024_1_1.png" align="middle" width="400px" />
                        <figcaption>Fig 26: max_ray_depth = 1 (CBbunny.dae)</figcaption>
                    </td>
                </tr>
                <tr align="center">
                    <td>
                        <img src="images/task4/m2_1024_1_2.png" align="middle" width="400px" />
                        <figcaption>Fig 27: max_ray_depth = 2 (CBbunny.dae)</figcaption>
                    </td>
                    <td>
                        <img src="images/task4/m3_1024_1_3.png" align="middle" width="400px" />
                        <figcaption>Fig 28: max_ray_depth = 3 (CBbunny.dae)</figcaption>
                    </td>
                </tr>
                <tr align="center">
                    <td>
                        <img src="images/task4/m100_1024_1_100.png" align="middle" width="400px" />
                        <figcaption>Fig 29: max_ray_depth = 100 (CBbunny.dae)</figcaption>
                    </td>
                </tr>
            </table>
        </div>
        <br />
        <p>
            When the max_ray_depth = 0, we only get zero-bounce illumination, and the resulting image lacks the realistic lighting effects that come with global illumination.
            With max_ray_depth = 1, we start to see the equivalent of direct illumination, but it is still limited to only the first bounce of light.
            As the max ray depth increases, there are more areas that become lit by indirect illumination, thus the darker part of the image start to disappear gradually.
            Finally, setting max_ray_depth = 100 results in extremely realistic lighting effects, but it takes a very long time to render and has minimal benefit compared to a may_ray_depth of 2 or 3.
        </p>
        <br />

        <h3>
            Pick one scene and compare rendered views with various sample-per-pixel rates, including at least 1, 2, 4, 8, 16, 64, and 1024. Use 4 light rays.
        </h3>
        <p>
            The images below were rendered with parameters: <code>-s [1, 2, 4, 8, 16, 64, 1024] -l 4 -m 6</code>.
        </p>
        <!-- Example of including multiple figures -->
        <div align="middle">
            <table style="width:100%">
                <tr align="center">
                    <td>
                        <img src="images/task4/s1_1_4_6.png" align="middle" width="400px" />
                        <figcaption>Fig 30: 1 sample per pixel (CBspheres_lambertian.dae)</figcaption>
                    </td>
                    <td>
                        <img src="images/task4/s2_2_4_6.png" align="middle" width="400px" />
                        <figcaption>Fig 31: 2 samples per pixel (CBspheres_lambertian.dae)</figcaption>
                    </td>
                </tr>
                <tr align="center">
                    <td>
                        <img src="images/task4/s4_4_4_6.png" align="middle" width="400px" />
                        <figcaption>Fig 32: 4 samples per pixel (CBspheres_lambertian.dae)</figcaption>
                    </td>
                    <td>
                        <img src="images/task4/s8_8_4_6.png" align="middle" width="400px" />
                        <figcaption>Fig 33: 8 samples per pixel (CBspheres_lambertian.dae)</figcaption>
                    </td>
                </tr>
                <tr align="center">
                    <td>
                        <img src="images/task4/s16_16_4_6.png" align="middle" width="400px" />
                        <figcaption>Fig 34: 16 samples per pixel (CBspheres_lambertian.dae)</figcaption>
                    </td>
                    <td>
                        <img src="images/task4/s64_64_4_6.png" align="middle" width="400px" />
                        <figcaption>Fig 35: 64 samples per pixel (CBspheres_lambertian.dae)</figcaption>
                    </td>
                </tr>
                <tr align="center">
                    <td>
                        <img src="images/task4/s1024_1024_4_6.png" align="middle" width="400px" />
                        <figcaption>Fig 36: 1024 samples per pixel (CBspheres_lambertian.dae)</figcaption>
                    </td>
                </tr>
            </table>
        </div>
        <br />
        <p>
            When the sample-per-pixel rate is set to 1, the resulting image is very noisy and lacks detail. As we increase the sample-per-pixel rate to 2, we start to see a small improvement in image quality, but the image is still noisy.
            As we continue to increase the sample-per-pixel rate to 4, 8, 16, 64, and 1024, the image quality continues to improve with less noise and more detail being drawn in the shadows and highlights.
            Increasing the number of samples per pixel therefore makes the image clearer with extra samples helping to reduce the noise (as in anti-aliasing). However, increasing the sample-per-pixel rate also increases rendering time.
        </p>
        <br />


        <h2 align="middle">Part 5: Adaptive Sampling (20 Points)</h2>
        <!-- Explain adaptive sampling. Walk through your implementation of the adaptive sampling.
    Pick one scene and render it with at least 2048 samples per pixel. Show a good sampling rate image with clearly visible differences in sampling rate over various regions and pixels. Include both your sample rate image, which shows your how your adaptive sampling changes depending on which part of the image you are rendering, and your noise-free rendered result. Use 1 sample per light and at least 5 for max ray depth. -->

        <h3>
            Explain adaptive sampling. Walk through your implementation of the adaptive sampling.
        </h3>
        <p>
            <b>Big Idea:</b> <br />
            <br />
            Adaptive sampling is an algorithm that helps us to avoid using a fixed (high) number of samples across all pixels. Instead, it chooses to concentrate the limited sampling resources to the samples that are in the more difficult parts. With this algorithm, we are able to alleviate noise more efficiently by setting different numbers of samples for specific pixels.
            <br />

            <br />
            <b>Implementation:</b>
            <ol style="list-style-type: disc;">
                <li>
                    For each new sample we obtain, compute the illuminance from the radiance information
                </li>
                <li>
                    Keep a running sum of the illuminance $x_k$ and illuminance squared $x_k^2$
                </li>

                <li>
                    For every $samplesPerBatch$, compute the mean and variance of the samples’ illuminance computed so far, using the formula provided in the spec. Additionally, we check to see if the pixel already converged, if so, we break from the for loop (as we had enough samples for this pixel) and note the number of samples collected so far.
                </li>
                <li>
                    Otherwise, keep the loop running until we have ns_aa samples/converged
                </li>
                <li>
                    Eventually, we update sampleCountBuffer with the actual number of samples we collected.

                </li>

            </ol>

        </p>
        <br />

        <h3>
            Pick two scenes and render them with at least 2048 samples per pixel. Show a good sampling rate image with clearly visible differences in sampling rate over various regions and pixels. Include both your sample rate image, which shows your how your adaptive sampling changes depending on which part of the image you are rendering, and your noise-free rendered result. Use 1 sample per light and at least 5 for max ray depth.
        </h3>
        The images below were rendered with parameters: <code>-s 2048 -a 64 0.05 -l 1 -m 5</code>.

        <br />
        <br />
        <!-- Example of including multiple figures -->
        <div align="middle">
            <table style="width:100%">
                <tr align="center">
                    <td>
                        <img src="images/task5/banana.png" align="middle" width="400px" />
                        <figcaption>Fig 37: Rendered image (banana.dae)</figcaption>
                    </td>
                    <td>
                        <img src="images/task5/banana_rate.png" align="middle" width="400px" />
                        <figcaption>Fig 38: Sample rate image (banana.dae) </figcaption>
                    </td>
                </tr>
                <tr align="center">
                    <td>
                        <img src="images/task5/bunny.png" align="middle" width="400px" />
                        <figcaption>Fig 39: Rendered image (CBbunny.dae)</figcaption>
                    </td>
                    <td>
                        <img src="images/task5/bunny_rate.png" align="middle" width="400px" />
                        <figcaption>Fig 40: Sample rate image (CBbunny.dae)</figcaption>
                    </td>
                </tr>
            </table>


        </div>
        <br />


    </div>


    </o>


</body>
</html>
